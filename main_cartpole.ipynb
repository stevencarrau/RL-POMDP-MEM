{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries \n",
    "Initial packages and testing functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "from dqn import DQN\n",
    "from drqn import DRQN, Memory\n",
    "from reinforce import REINFORCE, PiApproximationWithNN, Baseline\n",
    "from reinforce_Buffer import REINFORCE as RF_Buffer, PiApproximationWithNN as Pi_Buffer, ReplayMemory\n",
    "from reinforce_Estimate import REINFORCE as RF_Est, PiApproximationWithNN as Pi_Est\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def test_DQN(env, run):\n",
    "    gamma = 1.0\n",
    "    return DQN(env, gamma, 1000, run)\n",
    "\n",
    "\n",
    "def test_DRQN(env, run):\n",
    "    gamma = 1.0\n",
    "    return DRQN(env, gamma, 1000, run)\n",
    "\n",
    "def test_reinforce(env,runs):\n",
    "    gamma = 1.\n",
    "    alpha = 3e-4\n",
    "\n",
    "    if 'tensorflow' in sys.modules:\n",
    "        import tensorflow as tf\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    pi = PiApproximationWithNN(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.n,\n",
    "        alpha)\n",
    "\n",
    "    B = Baseline(0.)\n",
    "\n",
    "    return REINFORCE(env, gamma, 1000, runs, pi, B)\n",
    "\n",
    "\n",
    "def test_reinforce_Buffer(env, mem_size, runs):\n",
    "    gamma = 1.\n",
    "    alpha = 3e-4\n",
    "\n",
    "    if 'tensorflow' in sys.modules:\n",
    "        import tensorflow as tf\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    pi = Pi_Buffer(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.n,\n",
    "        alpha,\n",
    "        mem_size)\n",
    "\n",
    "    B = Baseline(0.)\n",
    "\n",
    "    return RF_Buffer(env, gamma, 1000, runs, pi, B, mem_size)\n",
    "\n",
    "def test_reinforce_Estimate(env, mem_size, runs):\n",
    "    gamma = 1.\n",
    "    alpha = 3e-4\n",
    "\n",
    "    if 'tensorflow' in sys.modules:\n",
    "        import tensorflow as tf\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    pi = Pi_Est(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.n,\n",
    "        alpha,\n",
    "        mem_size)\n",
    "\n",
    "    B = Baseline(0.)\n",
    "\n",
    "    return RF_Est(env, gamma, 1000, runs, pi, B, mem_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce Approaches\n",
    "\n",
    "In this section we compare 3 policy classes:\n",
    "1. Memoryless\n",
    "2. A memory sequence buffer (2 and 5 observations) \n",
    "3. An memory-based estimate of the state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "----------------> Without Buffer: 0\n",
      "----------------> Without Buffer: 1\n",
      "----------------> Without Buffer: 2\n",
      "----------------> Without Buffer: 3\n",
      "----------------> Without Buffer: 4\n",
      "----------------> Without Buffer: 5\n",
      "----------------> Without Buffer: 6\n",
      "----------------> Without Buffer: 7\n",
      "----------------> Without Buffer: 8\n",
      "----------------> Without Buffer: 9\n",
      "***************************************\n",
      "***************************************\n",
      "----------------> With Buffer = 2: 0\n",
      "----------------> With Buffer = 2: 1\n",
      "----------------> With Buffer = 2: 2\n",
      "----------------> With Buffer = 2: 3\n",
      "----------------> With Buffer = 2: 4\n",
      "----------------> With Buffer = 2: 5\n",
      "----------------> With Buffer = 2: 6\n",
      "----------------> With Buffer = 2: 7\n",
      "----------------> With Buffer = 2: 8\n",
      "----------------> With Buffer = 2: 9\n",
      "***************************************\n",
      "***************************************\n",
      "----------------> With Buffer = 5: 0\n",
      "----------------> With Buffer = 5: 1\n",
      "----------------> With Buffer = 5: 2\n",
      "----------------> With Buffer = 5: 3\n",
      "----------------> With Buffer = 5: 4\n",
      "----------------> With Buffer = 5: 5\n",
      "----------------> With Buffer = 5: 6\n",
      "----------------> With Buffer = 5: 7\n",
      "----------------> With Buffer = 5: 8\n",
      "----------------> With Buffer = 5: 9\n",
      "***************************************\n",
      "***************************************\n",
      "----------------> With Estiamte: 0\n",
      "----------------> With Estiamte: 1\n",
      "----------------> With Estiamte: 2\n",
      "----------------> With Estiamte: 3\n",
      "----------------> With Estiamte: 4\n",
      "----------------> With Estiamte: 5\n",
      "----------------> With Estiamte: 6\n",
      "----------------> With Estiamte: 7\n",
      "----------------> With Estiamte: 8\n",
      "----------------> With Estiamte: 9\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "num_iter = 10\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "without_buffer = []\n",
    "print('***************************************')\n",
    "for q in range(num_iter):\n",
    "    print(\"----------------> Without Buffer: {}\".format(q))\n",
    "    training_progress = test_reinforce(env,q)\n",
    "    without_buffer.append(training_progress[0])\n",
    "    pi = training_progress[1]\n",
    "print('***************************************')\n",
    "without_buffer = np.mean(without_buffer, axis=0)\n",
    "    \n",
    "# Test REINFORCE_buffer size 2 and 5\n",
    "with_buffer2 = []\n",
    "print('***************************************')\n",
    "for q in range(num_iter):\n",
    "    print(\"----------------> With Buffer = 2: {}\".format(q))\n",
    "    training_progress = test_reinforce_Buffer(env, 2, q)\n",
    "    with_buffer2.append(training_progress[0])\n",
    "    pi_buff = training_progress[1]\n",
    "print('***************************************')\n",
    "with_buffer2 = np.mean(with_buffer2, axis=0)\n",
    "\n",
    "with_buffer5 = []\n",
    "print('***************************************')\n",
    "for q in range(num_iter):\n",
    "    print(\"----------------> With Buffer = 5: {}\".format(q))\n",
    "    training_progress = test_reinforce_Buffer(env, 5, q)\n",
    "    with_buffer5.append(training_progress[0])\n",
    "    pi_buff = training_progress[1]\n",
    "print('***************************************')\n",
    "with_buffer5 = np.mean(with_buffer5, axis=0)\n",
    "\n",
    "# Test REINFORCE_estimate\n",
    "with_est = []\n",
    "print('***************************************')\n",
    "for q in range(num_iter):\n",
    "    print(\"----------------> With Estiamte: {}\".format(q))\n",
    "    training_progress = test_reinforce_Estimate(env, 2, q)\n",
    "    with_est.append(training_progress[0])\n",
    "    pi_est = training_progress[1]\n",
    "print('***************************************')\n",
    "with_est = np.mean(with_est, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the experiment result\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(np.arange(len(without_buffer)), without_buffer, label='No Buffer')\n",
    "ax.plot(np.arange(len(with_buffer2)), with_buffer2, label='Buffer - Size 2')\n",
    "ax.plot(np.arange(len(with_buffer5)), with_buffer5, label='Buffer - Size 5')\n",
    "ax.plot(np.arange(len(with_est)), with_est, label='Estimated State')\n",
    "\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('G_0')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result discussion\n",
    "The policy that uses the state estimate from memory clearly outperforms both the memoryless and the memory sequence policies. The comparison is not technically an even one since the state estimate method uses domain knowledge that the other two policies do not have.\n",
    "The really poor performance of the memory sequences is likely a result of the _curse of history_ as the number of samples required explodes with each additional observation in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max G_0 83.0\n",
      "Max G_0 60.0\n",
      "Max G_0 84.0\n",
      "Max G_0 80.0\n",
      "Max G_0 90.0\n",
      "Max G_0 60.0\n",
      "Max G_0 54.0\n",
      "Max G_0 77.0\n",
      "Max G_0 60.0\n",
      "Max G_0 71.0\n",
      "Max G_0 199.0\n",
      "Max G_0 199.0\n"
     ]
    }
   ],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / N\n",
    "\n",
    "# # Test DQN\n",
    "dqn_list = []\n",
    "dqn_policies = []\n",
    "for q in range(num_iter):\n",
    "    dqn_rew, dqn_pi = test_DQN(env, q)\n",
    "    dqn_list.append(dqn_rew)\n",
    "    dqn_policies.append(dqn_pi)\n",
    "dqn_result = np.mean(dqn_list,axis=0)\n",
    "smoothed_dqn_result = running_mean(dqn_result, 10)\n",
    "#\n",
    "# Test DRQN\n",
    "drqn_list = []\n",
    "drqn_policies = []\n",
    "for q in range(num_iter):\n",
    "    drqn_rew, drqn_pi = test_DRQN(env, q)\n",
    "    drqn_list.append(drqn_rew)\n",
    "    drqn_policies.append(drqn_pi)\n",
    "drqn_result = np.mean(drqn_list, axis=0)\n",
    "smoothed_drqn_result = running_mean(drqn_result, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(smoothed_dqn_result)), smoothed_dqn_result, label='DQN_smoothed')\n",
    "ax.plot(np.arange(len(dqn_result)), dqn_result, label='DQN', color='red', alpha=0.3)\n",
    "ax.plot(np.arange(len(smoothed_drqn_result)), smoothed_drqn_result, label='DRQN_smoothed')\n",
    "ax.plot(np.arange(len(drqn_result)), drqn_result, label='DRQN', color='grey', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('G_0')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result discussion\n",
    "\n",
    "Clearly adding an LSTM memory layer for DRQN gives a policy that acheives greater performance for Cartpole than the memoryless case in DQN.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
